# Iris Example ML Service Configuration

ml:
  sources:
    # HTTP model - direct API call to a running ML service
    http_model:
      base_url: "http://localhost:8501"
      models:
        default:
          endpoint: "/predict"
          timeout: 30
          headers:
            Content-Type: "application/json"
    
    # Local artifact model - load model from local filesystem
    local_model:
      base_url: "http://localhost:8502"  # Fallback if loading fails
      models:
        default:
          endpoint: "/predict"
          source:
            type: "local_artifact"
            path: "./example/iris_example/models"
            startup_command: "python -m example.iris_example.iris_model_server"
            host: "localhost"
            port: 8502
            startup_delay: 5  # Seconds to wait for model to start
            environment:
              MODEL_PATH: "models/iris_model.pkl"
              PORT: "8502"